\documentclass[8pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{minted}
\usemintedstyle{vs}
\usepackage{enumitem}
\newcommand{\saskeyword}[1]{\textcolor{blue}{#1}}
\newenvironment{sasminted}{\VerbatimEnvi	ronment\begin{minted}[escapeinside=||,fontsize=\small,baselinestretch=1, samepage=true]{sas}}
 {\end{minted}}
 \usepackage{amsmath}
\usepackage{verbatim}
\usepackage[landscape]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}

% To make this come out properly in landscape mode, do one of the following
% 1.
%  pdflatex latexsheet.tex
%
% 2.
%  latex latexsheet.tex
%  dvips -P pdf  -t landscape latexsheet.dvi
%  ps2pdf latexsheet.ps


% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.1in,left=.1in,right=.1in,bottom=.1in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}

% Turn off header and footer
\pagestyle{empty}
 

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\renewcommand{\baselinestretch}{0.7} 
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}


% -----------------------------------------------------------------------

\begin{document}

\raggedright
\footnotesize
\begin{multicols}{4}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
     \Large{\textbf{CS3210 Cheat Sheet}} \\
\end{center}

\section{Chapter 2}
\subsection{Synchronization}
Locks, Semaphores (Binary/Mutex and Counting) - Wait and Signal, Condition Variables (Wait, Signal, Broadcast [Signal all]), Monitors (Mutex + CV), Barrier, Starvation, Deadlock, Messages

\subsection{Producer-Consumer}

\subsection{Readers-Writers}

\subsection{Lightswitch}
Provides sort of an exclusive control over the semaphore while there exists counts of this type.

\subsection{Turnstile}

\section{Chapter 3}
\subsection{Levels of Parallelism}
Single Processor: \{Bit, Instruction, Thread, Process\} Level
\smallbreak \noindent Multiple Processors: \{Shared, Distributed\} Memory

\subsubsection{Bit Level}
Word Size (16-bit, 32-bit, 64-bit)

\subsubsection{Instruction Level}
\begin{enumerate}
	\item Pipelining - split instruction execution into multiple stages, allow multiple instructions to occupy different stages in same clock cycle; max achievable speedup = number of pipeline stages
	\item Superscalar - duplicate pipelines, allow multiple instructions to pass through the same stage
\end{enumerate}

\subsubsection{Thread Level}
Simultaneous Multi-Threading: Hyperthreading, Run multiple (2) threads at the same time

\subsubsection{Process Level}
Multiple processes can be mapped to multiple processor cores

\subsubsection{Flynn's Parallel Architecture Taxonomy}
\begin{itemize}
	\item SISD
	\item SIMD - SSE, AVX instructions
	\item MISD - Space shuttle
	\item MIMD - Multiprocessor
\end{itemize}

\subsubsection{Memory Organization}
\underline{Distributed-Memory Multicomputers} - Memory in node is private, message-passing to exchange data
\smallbreak \noindent \underline{Hybrid (Distributed-Shared Memory)}
\smallbreak \noindent \underline{Shared-memory Multiprocessors} - Data-exchanges between nodes through shared variables
\begin{itemize}
	 \item \underline{Uniform Memory Access (UMA)}
				\begin{itemize}[label=$\ast$]
					\item Latency of accessing main memory is same for all processors
					\item Main memory is congregated at some other area separate from processors
				\end{itemize}
			\item \underline{Non-Uniform Memory Access (NUMA)}	[distributed SHARED-MEMORY]
				\begin{itemize}[label=$\ast$]
					\item Physically distributed memory of all processing elements combined to form a global shared-memory address space
					\item Access local memory is fater than remote memory for a processor $\rightarrow$ non-uniform access time
					\item Related: Cache Coherent NUMA (ccNUMA) - Each node has cache memory to reduce contention
				\end{itemize}
			\item \underline{Cache-only Memory Access (COMA)}
				\begin{itemize}[label=$\ast$]
					\item Only cache present, no memory
					\item Data migrates dynamically and continuously according to cache coherence scheme
				\end{itemize}
\end{itemize}

\subsection{Multicore Architecture}
Hierachical design, Pipelined design, Network-based design - Interconnection networks

\section{Chapter 4}
Limits parallelism: Dependencies, Overheads of parallelism (switching), synchronization

\subsection{Instruction Parallelism}
Flow dependency (RAW), Anti-dependency (WAR), Output dependency (WAW)

\subsection{Loop Parallelism}
If each loop iteration is independent, can execute in parallel (i.e. OpenMP)

\subsection{Data Parallelism}
Partition data, similar operations on each part
\smallbreak \noindent SIMD, SPMD (i.e. MPI)

\subsection{Task Parallelism}
Partition tasks, same data on through all tasks (i.e. different components of an SQL statement)

\subsection{Task Dependence Graph}
\begin{itemize}
	\item Critical Path Length = Minimum (slowest) completion time
	\item Degree of concurrency = Total Work / Critical Path Length
\end{itemize}

\section{Chapter 5}
\subsection{CPU Time (No memory miss)}
\begin{equation}
\textrm{Time}_{\textrm{user}}(A) = N_{\textrm{cycle}}(A) \times \textrm{Time}_{\textrm{cycle}}
\end{equation}
\begin{equation}
N_{\textrm{cycle}}(A) = \sum_{i=1}^{n} n_{i}(A) \times \textrm{CPI}_{i}
\end{equation}
\begin{equation}
\textrm{Time}_{\textrm{user}}(A) = N_{\textrm{instructions}}(A) \times \textrm{CPI}(A) \times \textrm{Time}_{\textrm{cycle}}
\end{equation}

\subsection{CPU Time (With memory miss)}
\subsubsection{Memory Access Time}
\begin{equation}
\begin{split}
\textrm{Time}_{\textrm{user}}(A) = \left( N_{\textrm{cycle}}(A) + N_{\textrm{mm\_cycle}}(A)  \right) \\ 
\times \textrm{Time}_{\textrm{cycle}}
\end{split}
\end{equation}
\textbf{Consider a one-level cache:}
\begin{equation}
N_{\textrm{mm\_cycle}}(A) = N_{\textrm{read\_cycle}}(A) + N_{\textrm{write\_cycle}}(A)
\end{equation}
\begin{equation}
\begin{split}
N_{\textrm{read\_cycle}}(A) = N_{\textrm{read\_op}}(A) \times R_{\textrm{read\_miss}}(A) \\
 \times N_{\textrm{miss\_cycles}}(A)
\end{split}
\end{equation}
\begin{equation}
\begin{split}
N_{\textrm{write\_cycle}}(A) = N_{\textrm{write\_op}}(A) \times R_{\textrm{write\_miss}}(A) \\
 \times N_{\textrm{miss\_cycles}}(A)
\end{split}
\end{equation}

\subsubsection{Refinement with Memory Access Time}
\begin{equation}
\begin{split}
\textrm{Time}_{\textrm{user}}(A)  = ( N_{\textrm{instr}}(A) \times \textrm{CPI}(A) + N_{\textrm{rw\_op}}(A) \\ \times R_{\textrm{rw\_miss}}(A) \times N_{\textrm{rw\_cycles}}(A) ) \times \textrm{Time}_{\textrm{cycle}}
\end{split}
\end{equation}


\subsubsection{Average Memory Access Time}
Average read access time = Time for read hit + Adjusted / Average Time for read miss penalty
\begin{equation}
\begin{split}
T_{\textrm{read\_access}}(A) = T_{\textrm{read\_hit}}(A) + R_{\textrm{read\_miss}}(A) \\
 \times T_{\textrm{read\_miss}}(A)
\end{split}
\end{equation}
\textbf{Two-level Cache example:}
\begin{equation}
\begin{split}
T_{\textrm{read\_access}}(A) = T_{\textrm{read\_hit}}^{L1}(A) + R_{\textrm{read\_miss}}^{L1}(A) \\
\times T_{\textrm{read\_miss}}^{L1}(A)
\end{split}
\end{equation}
\begin{equation}
\begin{split}
T_{\textrm{read\_miss}}^{L1}(A) = T_{\textrm{read\_hit}}^{L2}(A) + R_{\textrm{read\_miss}}^{L2}(A) \\
 \times T_{\textrm{read\_miss}}^{L2}(A)
\end{split}
\end{equation}
\textbf{Global Miss Rate:}
\begin{equation}
R_{\textrm{read\_miss}}^{L1}(A) \times R_{\textrm{read\_miss}}^{L2}(A)
\end{equation}

\subsection{MIPS, MFLOPS}
\begin{equation}
MIPS(A) = \frac{N_{\textrm{instr}}(A)}{\textrm{Time}_{\textrm{user}}(A) \times 10^{6}} = \frac{\textrm{clock\_frequency}}{CPI(A) \times 10^{6}}
\end{equation}
\begin{equation}
MFLOPS(A) = \frac{N_{\textrm{fl\_ops}}(A)}{\textrm{Time}_{\textrm{user}}(A) \times 10^{6}}
\end{equation}

\subsection{Parallel Execution Time}
\begin{itemize}
	\item $T_{p}(n)$ - time for $p$ processors to work on problem of size $n$
\end{itemize}
\begin{equation}
C_{p}(n) = p \times T_{p}(n)
\end{equation}
\begin{itemize}
	\item $C_{p}(n)$ - cost of a parallel program with input size $n$ executed on $p$ processors
	\item Parallel program is cost optimal if it executes the same total number of operations as the fastest sequential program
\end{itemize}
\begin{equation}
S_{p}(n) = \frac{T_{best\_seq}(n)}{T_{p}(n)}
\end{equation}
\begin{itemize}
	\item $S_{p}(n)$ is the speedup of the parallel program on $p$ processors
	\item Theoretically $S_{p}(n) \leq p$ always holds
	\item In practice $S_{p}(n) > p$ can occur due to better cache locality, early termination
\end{itemize}
\begin{equation}
E_{p}(n) = \frac{T_{*}(n)}{C_{p}(n)} = \frac{S_{p}(n)}{p} = \frac{T_{*}(n)}{p \times T_{p}(n)}
\end{equation}
\begin{itemize}
	\item Use $T_{*}(n)$ as a shorthand for $T_{best\_seq}(n)$
	\item Efficiency measures the actual degree of speedup performance achived compared to the maximum
	\item In an ideal speedup $S_{p}(n) = p \rightarrow E_{p}(n) = 1$
\end{itemize}

\subsection{Parallel Laws}
\subsubsection{Amdahl's Law}
\begin{itemize}
	\item Speedup of parallel execution is limited by the fraction of the algorithm that cannot be parallelized, $f$
	\item $f (0 \leq f \leq 1)$ - the sequential fraction
	\item "Fixed-workload" performance
\end{itemize}

\begin{equation}
S_{p}(n) = \frac{T_{*}(n)}{f \times T_{*}(n) + \frac{1-f}{p}T_{*}(n)} = \frac{1}{f + \frac{1-f}{p}} \leq \frac{1}{f}
\end{equation}
\begin{equation}
S_{p}(n) = \frac{p}{1 + (p-1) f}
\end{equation}

\subsubsection{Gustafson's Law}
\begin{itemize}
	\item In many computing problems, $f$ is not a constant
	\item Depends on problem size $n$: $f$ is a function of $n$, $f(n)$
	\item An effective parallel algorithm is: \begin{equation}
		\lim_{n\rightarrow\infty} f(n) = 0
	\end{equation}
	\item Thus speedup: \begin{equation}
		\lim_{n\rightarrow\infty} S_{p}(n) = \frac{p}{1 + (p-1)f(n)} = p
	\end{equation}
	\item In such cases, we can have \begin{equation}
		S_{p}(n) \leq p
	\end{equation}
\end{itemize}

\begin{equation}
S_{p}(n) = \frac{\tau_{f} + \tau_{v}(n, 1)}{\tau_{f} + \tau_{v}(n, p)}
\end{equation}

Assume parallel program is perfectly parallelizable (without overheads):
\begin{equation}
\tau_{v}(n, 1) = T^{*}(n) - \tau_{f} \  \textrm{and} \ \tau_{v}(n, p) = \frac{T^{*}(n) - \tau_{f}}{p}
\end{equation}
\begin{equation}
S_{p}(n) = \frac{\tau_{f} + T^{*}(n) - \tau_{f}}{\tau_{f} + \frac{T^{*}(n) - \tau_{f}}{p}} = \frac{\frac{\tau_{f}}{T*(n) - \tau_{f}} + 1}{\frac{\tau_{f}}{T^{*}(n) - \tau_{f}} + \frac{1}{p}}
\end{equation}
If $T^*(n)$ increase strongly monotonically with $n$, then \begin{equation}
\lim_{n\rightarrow\infty} S_{p}(n) = p
\end{equation}

\section{Chapter 6}
\subsection{Memory Consistency Models}
Relaxed Consistency: Only if instructions operate on different memory locations

\textbf{TSO [W $\rightarrow$ R]}: All processors see updates in the same order

\smallbreak \noindent \textbf{PC [W $\rightarrow$ R]}: Different processors can see updates in different orders; \textbf{Note:} Ordering should still be consistent for updates coming from the same processor. If P1 executes X $\rightarrow$ Y, if P2 saw Y, then P2 must have seen X. But if P1 executes X and P2 executes Y, if P3 sees X first, it is possible for P4 to see Y first instead

\smallbreak \noindent \textbf{PSO [W $\rightarrow$ R; W $\rightarrow$ W]}: Similar to TSO, processors see updates in same order

\subsection{Interconnection Networks}
\subsubsection{Direct Interconnect}
\begin{itemize}
	\item \textbf{Diameter} - maximum distance between any pair of nodes. Small diameter ensures small distances for message transmission.
	\item \textbf{Node Degree} - number of direct neighbours of node. Small node degree reduces the node hardware overhead.
	\item \textbf{Graph Degree} - maximum degree of a node in network G.
	\item \textbf{Bisection width} - minimum number of edges that must be removed to divide network into two equal halves. (Bottlenecks) capacity of network to transmit messages simultaneously.
	\item \textbf{Bisection bandwidth} - total bandiwth available between the two bisected portion of the network.
	\item \textbf{Node connectivity} - minimum number of nodes that must fail to disconnect the network. Determines the robustness of the network.
	\item \textbf{Edge connectivity} - minimum number of edges that must fail to disconnect the network. Determine number of independent paths between any pair of nodes.
\end{itemize}

\includegraphics[scale=0.29]{./metrics}

\subsubsection{Indirect Interconnect}
\underline{Bus Network} - only 1 pair communicate at a time
\smallbreak \noindent \underline{Crossbar Network} - $n \times m$ switches
\smallbreak \noindent \underline{Multistage Switching Network (Om, Bu, Base)}
\smallbreak \noindent \underline{Omega network}
\smallbreak \noindent \includegraphics[scale=0.2]{./omega}
\begin{itemize}
\item $n \times n$ Omega network has $\log n$ stages
					\item $\frac{n}{2}$ switches per stage
					\item Switch position: ($\alpha$, $i$)
					\item $\alpha$: position of switch within a stage
					\item $i$: stage number
					\item Edge between ($\alpha$, $i$) and ($\beta$, $i+1$) where
					\item $\beta = \alpha$ by a cyclic left bit shift
					\item $\beta = \alpha$ by a cyclic left bit shift + inversion of LSBit
\end{itemize}
\smallbreak \noindent \underline{Butterfly network}
\smallbreak \noindent \includegraphics[scale=0.2]{./butterfly}
\begin{itemize}
\item Should be same number of switches and stages as Omega
					\item Node ($\alpha$, $i$) connects to:
					\item ($\alpha$, $i+1$), straight edge
					\item ($\alpha'$, $i$), $\alpha$ and $\alpha'$ differ in the $(i+1)$th bit from the left, i.e. cross edge
\end{itemize}
\smallbreak \noindent \underline{Baseline network}
\smallbreak \noindent \includegraphics[scale=0.2]{./baseline}

\subsection{Routing}
\subsubsection{Classification}
\underline{\textbf{Path length:}} \textbf{Minimal} or \textbf{Non-minimal} routing: whether shortest path is always chosen

\smallbreak \noindent \underline{\textbf{Adaptivity:}} \textbf{Deterministic}: Always same path for same pair of (source, destination) node; \textbf{Adaptive}: May take into account network status and adapt accordingly, e.g. avoid congested path, avoid dead nodes, etc.

\subsubsection{XY Routing for 2D Mesh}
\begin{itemize}
	\item $(X_{src}, Y_{src}) \rightarrow (X_{dst}, Y_{dst})$
	\item Move in X direction until $X_{src} == X_{dst}$
	\item Move in Y direction until $Y_{src} == Y_{dst}$
\end{itemize}

\subsubsection{E-Cube Routing for Hypercube}
\begin{itemize}
	\item $(\alpha_{n-1}, \alpha_{n-2}, \dots, \alpha_{1}, \alpha_{0}) \rightarrow (\beta_{n-1}, \beta_{n-2}, \dots, \beta_{1}, \beta_{0})$
	\item Start from MSB to LSB (or LSB to MSB)
	\item Find first different bit
	\item Go to the neighboring node with the bit corrected
	\item \textbf{At most n hops}
\end{itemize}

\subsubsection{XOR-Tag Routing for Omega Network}
\begin{itemize}
	\item Let $T = $ Source Id $\oplus$ Destination Id
	\item At stage-$k$ (from left to right):
	\item Go straight if bit $k$ of T is 0
	\item Crossover if bit $k$ of T is 1
\end{itemize}

\section{Chapter 7}
\subsection{Data Distribution}
\includegraphics[scale=0.5]{./blockandcyclic}
\includegraphics[scale=0.6]{./block-cyclic}
\subsubsection{Checkerboard}
\includegraphics[scale=0.5]{./checkerboardblockandcyclic}
\includegraphics[scale=0.6]{./checkerboardblockcyclic}

\subsection{Communication Operations}
\subsubsection{Single Transfer}

\subsubsection{Gather and Scatter}
\includegraphics[scale=0.45]{./gatherscatter}

\subsubsection{Single Broadcast}
\includegraphics[scale=0.45]{./singlebroadcast}

\subsubsection{Multi-Broadcast}
\includegraphics[scale=0.45]{./multibroadcast}

\subsubsection{Single-accumulation (gath, reduction)}
\includegraphics[scale=0.45]{./singleaccumulation}

\subsubsection{Multi-accumulation}
\includegraphics[scale=0.45]{./multiaccumulation}

\subsubsection{Total Exchange}
\includegraphics[scale=0.45]{./totalexchange}

\subsection{Duality}
Two communication operations is a duality if the same spanning tree can be used for both operations.

\includegraphics[scale=0.45]{./specialisation}

\section{Chapter 8}
\underline{Blocking}: Does not return until resource is safe to be reused or modified (Safe and easier programming)
\smallbreak \noindent \underline{Non-blocking}: Returns before it may be safe. (Hide communication overhead)
\smallbreak \noindent \underline{Buffered}: System buffer present. If blocking and sending, blocks until written to system send buffer. (Finite buffer management)
\smallbreak \noindent \underline{Non-Buffered}: System buffer absent. If blocking and sending, blocks until data is sent over (wait for a corresponding receive if no read buffer). (Idling)

\subsection{MPI Operation Semantics}
\subsubsection{Local view}
\underline{Blocking}: Return indicates safe to reuse resources
\smallbreak \noindent \underline{Non-blocking}: May return before operation completes, and before safe to reuse

\subsubsection{Global view}
\underline{Synchronous}: Communication operation does not complete before both processes have started their communication operation (needs sync / agreement to transfer data, only commences upon both "start")

\smallbreak \noindent \underline{Asynchronous}: Sender can execute communication operation without any coordination from receiver (e.g. data transfer can commence and place inside receiver system buffer before any explicit "receive")

\section{Chapter 9}
\begin{minted}{c++}
size_t total_num_threads = gridDim.x 
* gridDim.y * gridDim.z * blockDim.x 
* blockDim.y * blockDim.z;

size_t block_index_in_grid = blockIdx.x 
* (gridDim.y * gridDim.z) + blockIdx.y 
* (gridDim.z) + blockIdx.z;

size_t thread_index_in_block = 
threadIdx.x * (blockDim.y * blockDim.z)
+ threadIdx.y * (blockDim.z) + threadIdx.z;

size_t thread_id = block_index_in_grid 
* (blockDim.x * blockDim.y * blockDim.z)
+ thread_index_in_block;
\end{minted}
Coalesce Access to Global Memory into minimal memory transactions

\section{Chapter 10}
Foster's Design Methodology: Partionining, Communication, Agglomeration, Mapping


\rule{0.3\linewidth}{0.25pt}
\scriptsize
Copyright \copyright\ 2018 Edmund Mok

\end{multicols}
\end{document}
